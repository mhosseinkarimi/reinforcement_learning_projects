{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My RL Exploration","text":"<p>Here, I will share my journey of RL and my exploration through the field (pun intended) and hopefully, you will find it interesting too! </p>"},{"location":"#overview","title":"Overview","text":"<p>I will focus on the basics of Reinforcement Learning including but not limited to:</p> <ul> <li>Introduction to the Basic Concepts:<ul> <li>Agents, Action and State spaces</li> <li>Value Functions, Q-Values, Reward, and Reutrn</li> <li>Markov Decision Process and Policy</li> </ul> </li> <li>Tabular RL:<ul> <li>Policy Iteration, Value Iteration and Dynamic Programming</li> <li>Monte-Carlo Methods</li> <li>SARSA</li> <li>Temporal Difference</li> <li>Q-Leanring</li> </ul> </li> <li>Approximate Solution Methods<ul> <li>On-Policy Prediction &amp; Control</li> <li>Off-Policy Methods</li> <li>Policy Gradient Methods</li> </ul> </li> </ul> <p>And more to come ... </p>"},{"location":"#resources","title":"Resources","text":"<p>The main resources that I will follow during this course are the following:</p> <ol> <li>COMP 579: Reinforcement Learining lectures presented at McGill University by amazing Dr. Doina Precup.</li> <li>Reinforcement Learning Online Lectures presented at Sharif University of Technology (in Farsi) by Dr. Mohammadhossein Rohban.</li> <li>Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.</li> </ol>"},{"location":"introduction/","title":"Introductoin","text":""},{"location":"introduction/#what-is-reinforcement-learning","title":"What is Reinforcement Learning?","text":"<p>First things first, what is reinforcement learning!? Reinforcement learning relies on the idea of learning by interacting with an environment. The core concept of interaction with and environment and learning through the feedback recieved in this process, shapes the idea of reinforcement learning.</p> <p></p> Image taken from Reinforcement Learning: An Introduction by Sutton &amp; Barto <p>To formulate this method of learning, first, we need to define a few concepts: </p> <ul> <li> <p>Agent: The actor in the environment which acts upon the environment and recieves the feedback which it uses to reinforce positive behavior (wich larger rewards from the environment).</p> </li> <li> <p>Environment: Pretty much the closest thing you can have to a teacher in this world! But it teaches you through experimentation and implicit rewards (or punishments) in its feedback to the agent. At each moment, the agent has a state in the environment and can apply an action and recieve a feedback from the environment.</p> </li> <li> <p>State: The state can represent what the agent knows about its position regarding the environment. it can be measurements from a sensor to the location in a maze. Simply it represents where (how) you are with respect to your environment.</p> </li> <li> <p>Action: An action is what the agent decides to do at each step. It\u2019s the outcome of its decision-making process and directly affects the environment and the agent\u2019s next state. The set of possible actions can be simple, like moving left or right, or more complex, like adjusting the speed of a robot or choosing a word in a conversation.</p> </li> <li> <p>Reward: Reward is the feedback the agent gets from the environment after taking an action. It's just a number that tells the agent how good or bad its action was. The agent uses this number to learn what to do over time. The reward can be immediate or show up later, which makes learning tricky sometimes.</p> </li> </ul> <p>Now that we have the basic pieces of interaction, we can move toward how the agent uses them to reason and improve its decisions.</p>"},{"location":"introduction/#how-to-evaluate-the-performance","title":"How to Evaluate the Performance?","text":"<p>We saw that unlike supervised learning, in reinforcement learning the reward is the only supervisory signal that the agent recieves from the environment. But just like in real life, instantaneous reward does not necessary say everything about an action. In a lot of problems rewards are often delayed or are very sparse. This makes the process of decision making incridibly challenging. In this section, we introduce a few alternative and more robust metrics for performance in the long run.</p>"},{"location":"introduction/#return","title":"Return","text":"<p>A single reward is often not enough. What really matters is the total reward the agent can collect in the future \u2014 this is called the return. It helps the agent understand the long-term effect of its actions.</p> <p>The return at time step \\( t \\) is usually written as:</p> <p>$ G_t = r_{t+1} + r_{t+2} + r_{t+3} + \\cdots + r_T $</p> <p>In tasks that go on forever, we use a discount factor $\\gamma \\in [0, 1] $ to slowly reduce the importance of future rewards:</p> <p>$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $</p> <p>This makes sure the total return stays reasonable and helps focus more on near-future rewards.</p>"},{"location":"introduction/#value-function","title":"Value Function","text":"<p>The value function tells us how good it is for the agent to be in a certain state. It's the expected return if the agent starts from that state and follows its strategy (called a policy):</p> <p>$ V(s) = \\mathbb{E}[G_t \\mid s_t = s] $</p> <p>In simple words, it helps the agent know which states are better in the long run.</p>"},{"location":"introduction/#action-value-function","title":"Action-Value Function","text":"<p>The action-value function goes one step further. Instead of just looking at a state, it looks at both the state and the action. It tells the agent how good it is to take a specific action in a given state:</p> <p>$ Q(s, a) = \\mathbb{E}[G_t \\mid s_t = s, a_t = a] $</p> <p>This is very useful when the agent needs to decide between several possible actions in the same state. </p>"},{"location":"introduction/#how-can-we-model-the-decision-making-process","title":"How can we Model the Decision-making Process?","text":""},{"location":"introduction/#sequences-of-interaction","title":"Sequences of Interaction","text":"<p>In reinforcement learning, what the agent does now affects what happens next. The agent interacts with the environment step by step. At each time step \\(t\\), the agent:</p> <ul> <li>observes a state \\(s_t\\)</li> <li>takes an action \\(a_t\\)</li> <li>receives a reward \\(r_{t+1}\\)</li> <li>transitions to a new state \\(s_{t+1}\\)</li> </ul> <p>This forms a sequence like:</p> \\[ s_0, a_0, r_1, s_1, a_1, r_2, s_2, \\dots \\] <p>These sequences are important because the goal of the agent is to choose actions that lead to better rewards over time. This sequence also means that the agent must balance what it knows now with how its current decisions affect future states and rewards.</p>"},{"location":"introduction/#transition-probability","title":"Transition Probability","text":"<p>To model how the environment reacts to the agent\u2019s actions, we use the concept of a transition probability. This tells us the probability of ending up in a certain next state and receiving a certain reward, given the current state and action.</p> <p>Mathematically, this is written as:</p> \\[ P(s', r \\mid s, a) \\] <p>Where:</p> <ul> <li>\\(s\\): current state  </li> <li>\\(a\\): action taken in state \\(s\\) </li> <li>\\(s'\\): resulting next state  </li> <li>\\(r\\): reward received after taking action \\(a\\) in state \\(s\\)</li> </ul> <p>This probability describes the dynamics of the environment. If the outcome is always the same, we call it deterministic. Otherwise, it is stochastic.</p> <p>When the number of states is finite, these transition probabilities can be represented in a transition probability matrix. For a given action \\(a\\), this matrix \\(P_a\\) holds values:</p> \\[ P_a(s, s') = \\mathbb{P}[s_{t+1} = s' \\mid s_t = s, a_t = a] \\] <p>Each row of this matrix gives the probability distribution over next states, given the current state and action. It forms the basis for computing expected returns, value functions, and planning.</p> <p>Markov Process</p> <p>A Markov process is a stochastic process where the future depends only on the current state \u2014 not on the sequence of events that preceded it. This is known as the Markov property:</p> \\[ \\mathbb{P}[s_{t+1} \\mid s_t, s_{t-1}, \\dots, s_0] = \\mathbb{P}[s_{t+1} \\mid s_t] \\] <p>A Markov process without actions or rewards is called a Markov chain. When we add actions and rewards, it becomes a Markov Decision Process (MDP).</p>"},{"location":"introduction/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>A Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making in environments that satisfy the Markov property.</p> <p>An MDP is defined by:</p> <ul> <li>A set of states \\(S\\)</li> <li>A set of actions \\(A\\)</li> <li>A transition probability function \\(P(s', r \\mid s, a)\\)</li> <li>A reward function \\(R(s, a)\\) or \\(R(s, a, s')\\)</li> <li>A discount factor \\(\\gamma \\in [0, 1]\\)</li> </ul> <p>In this setup, the agent interacts with the environment over time and chooses actions based on a policy \\(\\pi(a \\mid s)\\), which defines the probability of taking action \\(a\\) in state \\(s\\).</p> <p>The goal of the agent is to find a policy that maximizes the expected return:</p> \\[ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots \\] <p>This compact structure makes MDPs very useful for developing and analyzing reinforcement learning algorithms.</p>"},{"location":"introduction/#policy","title":"Policy","text":"<p>Now that we\u2019ve seen how an agent interacts with the environment and how the environment behaves probabilistically, we can talk about what actually guides the agent\u2019s behavior \u2014 the policy.</p> <p>A policy is the agent\u2019s strategy for choosing actions based on the current state. It defines the agent's behavior at every point in time.</p>"},{"location":"introduction/#deterministic-policy","title":"Deterministic Policy","text":"<p>A deterministic policy maps each state directly to a specific action:</p> \\[ \\pi(s) = a \\] <p>This means that whenever the agent is in state \\(s\\), it always chooses action \\(a\\).</p>"},{"location":"introduction/#stochastic-policy","title":"Stochastic Policy","text":"<p>In many settings, it is useful (or even necessary) to consider stochastic policies. These assign probabilities to each action given a state:</p> \\[ \\pi(a \\mid s) = \\mathbb{P}[a_t = a \\mid s_t = s] \\] <p>This is the probability that the agent chooses action \\(a\\) when in state \\(s\\). Stochastic policies are especially useful for exploration, policy gradients, and environments with uncertainty.</p>"},{"location":"introduction/#policy-evaluation","title":"Policy Evaluation","text":"<p>To evaluate how good a policy is, we use value functions:</p> <ul> <li>The state-value function for a policy \\(\\pi\\):</li> </ul> <p>$$   V^\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]   $$</p> <p>This gives the expected return starting from state \\(s\\) and following policy \\(\\pi\\) thereafter.</p> <ul> <li>The action-value function for a policy \\(\\pi\\):</li> </ul> <p>$$   Q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]   $$</p> <p>This gives the expected return after taking action \\(a\\) in state \\(s\\), and then following policy \\(\\pi\\) from the next state.</p>"},{"location":"introduction/#policy-improvement","title":"Policy Improvement","text":"<p>The goal of reinforcement learning is to improve the policy over time. A better policy is one that gives higher expected returns.</p> <p>In many RL algorithms (like policy iteration or actor-critic methods), we update the policy by comparing value estimates and adjusting the action choices to increase the expected return. This is often done by:</p> <ul> <li>Making the policy more likely to choose actions with higher \\(Q^\\pi(s, a)\\)</li> <li>Reducing the likelihood of actions with lower value</li> </ul> <p>Over time, this leads to optimal policies:</p> \\[ \\pi^* = \\arg\\max_\\pi V^\\pi(s), \\quad \\forall s \\in S \\] <p>This optimal policy achieves the highest expected return from any state.</p>"},{"location":"tabular_rl/","title":"Tabular rl","text":""},{"location":"tabular_rl/#how-can-we-model-the-decision-making-process","title":"How can we Model the Decision-making Process?","text":""},{"location":"tabular_rl/#sequences-of-interaction","title":"Sequences of Interaction","text":"<p>In reinforcement learning, what the agent does now affects what happens next. The agent interacts with the environment step by step. At each time step \\(t\\), the agent:</p> <ul> <li>observes a state \\(s_t\\)</li> <li>takes an action \\(a_t\\)</li> <li>receives a reward \\(r_{t+1}\\)</li> <li>transitions to a new state \\(s_{t+1}\\)</li> </ul> <p>This forms a sequence like:</p> <p>\\(s_0, a_0, r_1, s_1, a_1, r_2, s_2, \\dots\\)</p> <p>These sequences are important because the goal of the agent is to choose actions that lead to better rewards over time. This sequence also means that the agent must balance what it knows now with how its current decisions affect future states and rewards.</p>"},{"location":"tabular_rl/#transition-probability","title":"Transition Probability","text":"<p>To model how the environment reacts to the agent\u2019s actions, we use the concept of a transition probability. This tells us the probability of ending up in a certain next state and receiving a certain reward, given the current state and action.</p> <p>This is written as:</p> <p>\\(P(s', r \\mid s, a)\\) </p> <p>Where:</p> <ul> <li>\\(s\\): current state  </li> <li>\\(a\\): current action  </li> <li>\\(s'\\): next state  </li> <li>\\(r\\): reward received after taking action \\(a\\) in state \\(s\\)</li> </ul> <p>This defines the dynamics of the environment. If the environment is deterministic, the next state and reward are always the same for a given state-action pair. If it\u2019s stochastic, the outcomes are uncertain and described by probabilities.</p>"},{"location":"tabular_rl/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>When the next state and reward depend only on the current state and action (and not the full history), we say the environment has the Markov property.</p> <p>A Markov Decision Process (MDP) is a common way to formalize reinforcement learning problems. An MDP is defined by:</p> <ul> <li>A set of states \\(S\\)</li> <li>A set of actions \\(A\\)</li> <li>A transition function \\(P(s', r \\mid s, a)\\)</li> <li>A reward function \\(R(s, a)\\) or \\(R(s, a, s')\\)</li> <li>A discount factor \\(\\gamma \\in [0, 1]\\)</li> </ul> <p>The agent\u2019s objective in an MDP is to learn a policy (a rule for choosing actions) that maximizes the expected return:</p> <p>\\(G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots\\)</p> <p>The Markov property helps simplify learning and planning, and most RL algorithms assume the environment follows this structure.</p>"}]}